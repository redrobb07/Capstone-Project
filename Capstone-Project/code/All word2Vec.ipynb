{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# If you want to use gensim's data, import their downloader\n",
    "# and load it.\n",
    "\n",
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "\n",
    "# If you have your own iterable corpus of cleaned data, you can \n",
    "# read it in as corpus and pass that in.\n",
    "\n",
    "# Train a model! \n",
    "model = Word2Vec(corpus,      # Corpus of data.\n",
    "                 size=100,    # How many dimensions do you want in your word vector?\n",
    "                 window=5,    # How many \"context words\" do you want?\n",
    "                 min_count=1, # Ignores words below this threshold.\n",
    "                 sg=1,        # SG = 1 uses SkipGram, SG = 0 uses CBOW (default).\n",
    "                 workers=4)   # Number of \"worker threads\" to use (parallelizes process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you'd like to do with your data!\n",
    "model.train(df['tokens'],\n",
    "           total_examples=model.corpus_count,\n",
    "           epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../results/w2v_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using gensim corpus then find the vectors of the words I'm looking for in the MFD and find those in model\n",
    "# 1) Use the word to vec and lda to identify themes and word embeddings, train a model to predict for democrat and republican..\n",
    "# 2) Hard code the search for terms and figure out how to get the common foundations people fit into and test the accuracy of each model against each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12453155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use w2v on the tokens columns to find the words. Then create 2000 columns, 1 for each word I'm looking for. Add a count to the column if the word is in the text.\n",
    "# Then maybe count up the number for each foundation after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a582d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(size=100,    # How many dimensions do you want in your word vector?\n",
    "                 window=5,    # How many \"context words\" do you want?\n",
    "                 min_count=1, # Ignores words below this threshold.\n",
    "                 sg=1,        # SG = 1 uses SkipGram, SG = 0 uses CBOW (default).\n",
    "                 workers=4)\n",
    "\n",
    "X_train_vocab = model.build_vocab(X_train)\n",
    "\n",
    "print('\\n Training the model \\n')\n",
    "model.train(X_train, total_examples=len(X_train), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two datasets must be the same size\n",
    "# This section is for training data\n",
    "max_dataset_size = len(model.wv.vectors)\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "lr.fit(model.wv.vectors, y_train[:max_dataset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eefe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e340740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ethen8181.github.io/machine-learning/keras/text_classification/word2vec_text_classification.html\n",
    "# Cannot seem to import transformers correctly and not sure why????\n",
    "# from transformers import \n",
    "# w2v_pipe = Pipeline([\n",
    "#     ('w2v', model),\n",
    "#     ('lr', LogisticRegression(C=100))\n",
    "# ])\n",
    "# w2v_pipe.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
